{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spectre: Weight Outlier Detection & Integrity Fingerprint Engine\n",
        "\n",
        "This notebook demonstrates how to use Spectre to scan model checkpoints for weight-level anomalies and tampering.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Add parent directory to path\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "\n",
        "from spectre.core.config import Config\n",
        "from spectre.core.pipeline import Pipeline\n",
        "from spectre.core.output import generate_all_outputs\n",
        "from spectre.io.loader import get_checkpoint_info\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set the path to your model checkpoint and configure the scan.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model checkpoint path\n",
        "MODEL_PATH = Path(\"/path/to/your/model.safetensors\")  # Update this!\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = Path(\"./scan_results\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Model ruleset (gpt_like, llama_like, mistral_like, phi_like)\n",
        "RULESET = \"gpt_like\"\n",
        "\n",
        "# Whether to save visualizations\n",
        "SAVE_VISUALS = True\n",
        "TOP_K_VISUALS = 20\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Configuration\n",
        "\n",
        "Create a configuration object. You can either load from YAML or create programmatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Load from YAML file\n",
        "# config = Config(config_path=Path(\"../scan_config.yaml\"))\n",
        "\n",
        "# Option 2: Create programmatically\n",
        "config = Config()\n",
        "config.set(\"model.path\", str(MODEL_PATH))\n",
        "config.set(\"model.ruleset\", RULESET)\n",
        "config.set(\"output.dir\", str(OUTPUT_DIR))\n",
        "config.set(\"output.save_visuals\", SAVE_VISUALS)\n",
        "config.set(\"output.topk_visuals\", TOP_K_VISUALS)\n",
        "\n",
        "# Enable/disable specific detectors\n",
        "config.set(\"svd.enabled\", True)\n",
        "config.set(\"interlayer.enabled\", True)\n",
        "config.set(\"distribution.enabled\", True)\n",
        "config.set(\"robust.enabled\", True)\n",
        "config.set(\"energy.enabled\", True)\n",
        "config.set(\"rmt.enabled\", True)\n",
        "config.set(\"spectrogram.enabled\", True)\n",
        "config.set(\"gsp.enabled\", True)\n",
        "config.set(\"tda.enabled\", True)\n",
        "config.set(\"sequence_cp.enabled\", True)\n",
        "config.set(\"ot.enabled\", True)\n",
        "config.set(\"multiview.enabled\", False)  # Default disabled\n",
        "config.set(\"conformal.enabled\", True)\n",
        "\n",
        "print(f\"Configuration loaded:\")\n",
        "print(f\"  Model: {config.get('model.path')}\")\n",
        "print(f\"  Ruleset: {config.get('model.ruleset')}\")\n",
        "print(f\"  Output: {config.get('output.dir')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check Model Info\n",
        "\n",
        "Get basic information about the checkpoint before scanning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if MODEL_PATH.exists():\n",
        "    model_info = get_checkpoint_info(MODEL_PATH)\n",
        "    print(f\"Model Information:\")\n",
        "    print(f\"  Path: {model_info['path']}\")\n",
        "    print(f\"  Size: {model_info['size_bytes'] / 1e9:.2f} GB\")\n",
        "    print(f\"  Format: {model_info['format']}\")\n",
        "    print(f\"  Number of tensors: {model_info['num_tensors']}\")\n",
        "    print(f\"  Total parameters: {model_info['total_params']:,}\")\n",
        "else:\n",
        "    print(f\"Warning: Model path does not exist: {MODEL_PATH}\")\n",
        "    print(\"Please update MODEL_PATH in the configuration cell above.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Pipeline\n",
        "\n",
        "Initialize and run the full pipeline: load → map → detect → score → output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize pipeline\n",
        "pipeline = Pipeline(config)\n",
        "\n",
        "print(\"Starting Spectre scan...\")\n",
        "print(f\"Model: {MODEL_PATH}\")\n",
        "print(f\"Output: {OUTPUT_DIR}\")\n",
        "print(f\"Ruleset: {RULESET}\")\n",
        "print()\n",
        "\n",
        "# Run pipeline\n",
        "results = pipeline.run(checkpoint_path=MODEL_PATH)\n",
        "\n",
        "print(\"\\nScan complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View Results Summary\n",
        "\n",
        "Display the scan summary with risk flags and suspect tensors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary = results.get(\"summary\", {})\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Scan Summary\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total tensors: {summary.get('total_tensors', 0)}\")\n",
        "print(f\"Ensemble sigma: {summary.get('ensemble_sigma', 0.0):.4f}\")\n",
        "print(f\"Overall flag: {summary.get('flag', 'GREEN')}\")\n",
        "print()\n",
        "\n",
        "# Flag distribution\n",
        "flag_counts = summary.get(\"flag_counts\", {})\n",
        "if flag_counts:\n",
        "    print(\"Flag distribution:\")\n",
        "    for flag, count in flag_counts.items():\n",
        "        print(f\"  {flag}: {count}\")\n",
        "    print()\n",
        "\n",
        "# Top suspects\n",
        "suspects = summary.get(\"suspects\", [])\n",
        "if suspects:\n",
        "    print(f\"Top {min(10, len(suspects))} suspects:\")\n",
        "    for i, suspect in enumerate(suspects[:10], 1):\n",
        "        print(f\"  {i}. {suspect['name'][:60]}\")\n",
        "        print(f\"     Role: {suspect['role']}, Layer: {suspect.get('layer_idx', 'N/A')}\")\n",
        "        print(f\"     Score: {suspect['ensemble_score']:.4f}, Flag: {suspect['flag']}\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Analyze Features\n",
        "\n",
        "Load the generated features CSV for detailed analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load features CSV\n",
        "features_path = OUTPUT_DIR / \"features.csv\"\n",
        "if features_path.exists():\n",
        "    df_features = pd.read_csv(features_path)\n",
        "    print(f\"Loaded {len(df_features)} tensor features\")\n",
        "    print(f\"\\nColumns: {len(df_features.columns)}\")\n",
        "    print(f\"\\nFirst few rows:\")\n",
        "    display(df_features.head())\n",
        "    \n",
        "    # Show feature statistics\n",
        "    print(\"\\nFeature statistics:\")\n",
        "    numeric_cols = df_features.select_dtypes(include=[np.number]).columns\n",
        "    display(df_features[numeric_cols].describe())\n",
        "else:\n",
        "    print(f\"Features CSV not found: {features_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Per-Layer Summary\n",
        "\n",
        "Analyze results by layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load per-layer summary\n",
        "layer_summary_path = OUTPUT_DIR / \"per_layer_summary.csv\"\n",
        "if layer_summary_path.exists():\n",
        "    df_layers = pd.read_csv(layer_summary_path)\n",
        "    print(f\"Loaded {len(df_layers)} layer summaries\")\n",
        "    display(df_layers)\n",
        "    \n",
        "    # Plot layer scores\n",
        "    if len(df_layers) > 0:\n",
        "        fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
        "        \n",
        "        # Average ensemble score by layer\n",
        "        axes[0].plot(df_layers['layer_idx'], df_layers['avg_ensemble_score'], 'b-o', linewidth=2, markersize=4)\n",
        "        axes[0].axhline(y=2.0, color='g', linestyle='--', label='GREEN threshold')\n",
        "        axes[0].axhline(y=3.0, color='r', linestyle='--', label='RED threshold')\n",
        "        axes[0].set_xlabel('Layer Index')\n",
        "        axes[0].set_ylabel('Average Ensemble Score')\n",
        "        axes[0].set_title('Average Ensemble Score by Layer')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Max ensemble score by layer\n",
        "        axes[1].plot(df_layers['layer_idx'], df_layers['max_ensemble_score'], 'r-o', linewidth=2, markersize=4)\n",
        "        axes[1].axhline(y=2.0, color='g', linestyle='--', label='GREEN threshold')\n",
        "        axes[1].axhline(y=3.0, color='r', linestyle='--', label='RED threshold')\n",
        "        axes[1].set_xlabel('Layer Index')\n",
        "        axes[1].set_ylabel('Max Ensemble Score')\n",
        "        axes[1].set_title('Max Ensemble Score by Layer')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(OUTPUT_DIR / \"layer_scores.png\", dpi=150, bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "else:\n",
        "    print(f\"Layer summary CSV not found: {layer_summary_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze Detector Scores\n",
        "\n",
        "Examine scores from individual detectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if pipeline.scorer:\n",
        "    # Get detector scores for top suspects\n",
        "    suspects = pipeline.scorer.get_suspects(top_k=10)\n",
        "    \n",
        "    if suspects:\n",
        "        # Create DataFrame of detector scores\n",
        "        detector_data = []\n",
        "        for suspect in suspects:\n",
        "            row = {\"name\": suspect[\"name\"][:50], \"ensemble\": suspect[\"ensemble_score\"]}\n",
        "            row.update(suspect.get(\"detector_scores\", {}))\n",
        "            detector_data.append(row)\n",
        "        \n",
        "        df_detectors = pd.DataFrame(detector_data)\n",
        "        \n",
        "        print(\"Detector scores for top suspects:\")\n",
        "        display(df_detectors)\n",
        "        \n",
        "        # Plot detector scores heatmap\n",
        "        if len(df_detectors) > 1:\n",
        "            detector_cols = [col for col in df_detectors.columns if col not in [\"name\", \"ensemble\"]]\n",
        "            if detector_cols:\n",
        "                import seaborn as sns\n",
        "                \n",
        "                plt.figure(figsize=(12, max(6, len(df_detectors) * 0.3)))\n",
        "                sns.heatmap(\n",
        "                    df_detectors[detector_cols].T,\n",
        "                    xticklabels=df_detectors[\"name\"],\n",
        "                    yticklabels=detector_cols,\n",
        "                    cmap=\"RdYlGn_r\",\n",
        "                    center=0,\n",
        "                    vmin=-5,\n",
        "                    vmax=5,\n",
        "                    cbar_kws={\"label\": \"Z-Score\"}\n",
        "                )\n",
        "                plt.title(\"Detector Z-Scores for Top Suspects\")\n",
        "                plt.xlabel(\"Tensor\")\n",
        "                plt.ylabel(\"Detector\")\n",
        "                plt.xticks(rotation=45, ha=\"right\")\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(OUTPUT_DIR / \"detector_scores_heatmap.png\", dpi=150, bbox_inches=\"tight\")\n",
        "                plt.show()\n",
        "else:\n",
        "    print(\"Scorer not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View Fingerprint JSON\n",
        "\n",
        "Load and display the full fingerprint JSON.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load fingerprint JSON\n",
        "fingerprint_path = OUTPUT_DIR / \"fingerprint_v2.json\"\n",
        "if fingerprint_path.exists():\n",
        "    with open(fingerprint_path, \"r\") as f:\n",
        "        fingerprint = json.load(f)\n",
        "    \n",
        "    print(\"Fingerprint structure:\")\n",
        "    print(f\"  Model: {fingerprint.get('model', {}).get('path', 'N/A')}\")\n",
        "    print(f\"  Summary: {list(fingerprint.get('summary', {}).keys())}\")\n",
        "    print(f\"  Per-tensor entries: {len(fingerprint.get('per_tensor', []))}\")\n",
        "    print(f\"  Detectors: {list(fingerprint.get('detectors', {}).keys())}\")\n",
        "    \n",
        "    # Display summary\n",
        "    print(\"\\nFull summary:\")\n",
        "    print(json.dumps(fingerprint.get(\"summary\", {}), indent=2))\n",
        "else:\n",
        "    print(f\"Fingerprint JSON not found: {fingerprint_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Analysis\n",
        "\n",
        "Perform custom analysis on the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View Generated Visualizations\n",
        "\n",
        "Display the visualizations that were generated during the scan.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "viz_dir = OUTPUT_DIR / \"visualizations\"\n",
        "if viz_dir.exists():\n",
        "    viz_files = list(viz_dir.glob(\"*.png\"))\n",
        "    print(f\"Found {len(viz_files)} visualization files:\")\n",
        "    for viz_file in sorted(viz_files):\n",
        "        print(f\"  - {viz_file.name}\")\n",
        "    \n",
        "    # Display key visualizations\n",
        "    key_viz = [\"similarity_heatmap.png\", \"zscore_heatmap.png\"]\n",
        "    \n",
        "    for viz_name in key_viz:\n",
        "        viz_path = viz_dir / viz_name\n",
        "        if viz_path.exists():\n",
        "            print(f\"\\n{viz_name}:\")\n",
        "            img = plt.imread(viz_path)\n",
        "            plt.figure(figsize=(14, 10))\n",
        "            plt.imshow(img)\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(viz_name.replace(\"_\", \" \").title())\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "    \n",
        "    # Show list of all generated visualizations\n",
        "    print(f\"\\nAll generated visualizations ({len(viz_files)} total):\")\n",
        "    for viz_file in sorted(viz_files):\n",
        "        print(f\"  - {viz_file.name}\")\n",
        "else:\n",
        "    print(f\"Visualizations directory not found: {viz_dir}\")\n",
        "    print(\"Set SAVE_VISUALS=True to generate visualizations\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detector Analysis Summary\n",
        "\n",
        "Analyze which detectors are contributing most to the anomaly scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if pipeline.scorer:\n",
        "    # Analyze detector contributions\n",
        "    detector_contributions = {}\n",
        "    \n",
        "    for tensor_name, detector_scores in pipeline.scorer.detector_scores.items():\n",
        "        for detector, score in detector_scores.items():\n",
        "            if detector not in detector_contributions:\n",
        "                detector_contributions[detector] = []\n",
        "            detector_contributions[detector].append(abs(score))\n",
        "    \n",
        "    # Compute statistics per detector\n",
        "    detector_stats = {}\n",
        "    for detector, scores in detector_contributions.items():\n",
        "        if scores:\n",
        "            detector_stats[detector] = {\n",
        "                \"mean\": np.mean(scores),\n",
        "                \"std\": np.std(scores),\n",
        "                \"max\": np.max(scores),\n",
        "                \"count\": len(scores),\n",
        "                \"high_anomaly_count\": sum(1 for s in scores if s >= 3.0)\n",
        "            }\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df_detector_stats = pd.DataFrame(detector_stats).T\n",
        "    df_detector_stats = df_detector_stats.sort_values(\"max\", ascending=False)\n",
        "    \n",
        "    print(\"Detector Statistics:\")\n",
        "    print(\"=\" * 60)\n",
        "    display(df_detector_stats)\n",
        "    \n",
        "    # Plot detector statistics\n",
        "    if len(df_detector_stats) > 0:\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "        \n",
        "        # Mean scores\n",
        "        axes[0, 0].bar(range(len(df_detector_stats)), df_detector_stats[\"mean\"])\n",
        "        axes[0, 0].set_xticks(range(len(df_detector_stats)))\n",
        "        axes[0, 0].set_xticklabels(df_detector_stats.index, rotation=45, ha=\"right\")\n",
        "        axes[0, 0].set_ylabel(\"Mean |Z-Score|\")\n",
        "        axes[0, 0].set_title(\"Mean Detector Scores\")\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Max scores\n",
        "        axes[0, 1].bar(range(len(df_detector_stats)), df_detector_stats[\"max\"], color=\"r\")\n",
        "        axes[0, 1].set_xticks(range(len(df_detector_stats)))\n",
        "        axes[0, 1].set_xticklabels(df_detector_stats.index, rotation=45, ha=\"right\")\n",
        "        axes[0, 1].set_ylabel(\"Max |Z-Score|\")\n",
        "        axes[0, 1].set_title(\"Max Detector Scores\")\n",
        "        axes[0, 1].axhline(y=3.0, color=\"r\", linestyle=\"--\", label=\"RED threshold\")\n",
        "        axes[0, 1].axhline(y=4.5, color=\"darkred\", linestyle=\"--\", label=\"HARD_RED threshold\")\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # High anomaly count\n",
        "        axes[1, 0].bar(range(len(df_detector_stats)), df_detector_stats[\"high_anomaly_count\"], color=\"orange\")\n",
        "        axes[1, 0].set_xticks(range(len(df_detector_stats)))\n",
        "        axes[1, 0].set_xticklabels(df_detector_stats.index, rotation=45, ha=\"right\")\n",
        "        axes[1, 0].set_ylabel(\"Count (|Z| >= 3.0)\")\n",
        "        axes[1, 0].set_title(\"High Anomaly Count per Detector\")\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Detector weights (from configuration)\n",
        "        detector_weights = pipeline.scorer.weights\n",
        "        weights_list = [detector_weights.get(det, 0.0) for det in df_detector_stats.index]\n",
        "        axes[1, 1].bar(range(len(df_detector_stats)), weights_list, color=\"g\")\n",
        "        axes[1, 1].set_xticks(range(len(df_detector_stats)))\n",
        "        axes[1, 1].set_xticklabels(df_detector_stats.index, rotation=45, ha=\"right\")\n",
        "        axes[1, 1].set_ylabel(\"Weight\")\n",
        "        axes[1, 1].set_title(\"Detector Weights in Ensemble\")\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(OUTPUT_DIR / \"detector_analysis.png\", dpi=150, bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"Scorer not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export Results Summary\n",
        "\n",
        "Export a summary of results for reporting or further analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export summary to JSON\n",
        "summary_export = {\n",
        "    \"model_path\": str(MODEL_PATH),\n",
        "    \"ruleset\": RULESET,\n",
        "    \"summary\": summary,\n",
        "    \"output_dir\": str(OUTPUT_DIR),\n",
        "    \"timestamp\": pd.Timestamp.now().isoformat()\n",
        "}\n",
        "\n",
        "export_path = OUTPUT_DIR / \"summary_export.json\"\n",
        "with open(export_path, \"w\") as f:\n",
        "    json.dump(summary_export, f, indent=2)\n",
        "\n",
        "print(f\"Summary exported to: {export_path}\")\n",
        "print(f\"\\nAll outputs saved to: {OUTPUT_DIR}\")\n",
        "print(f\"\\nGenerated files:\")\n",
        "print(f\"  - fingerprint_v2.json\")\n",
        "print(f\"  - features.csv\")\n",
        "print(f\"  - per_layer_summary.csv\")\n",
        "if SAVE_VISUALS:\n",
        "    print(f\"  - visualizations/ ({len(list((OUTPUT_DIR / 'visualizations').glob('*.png'))) if (OUTPUT_DIR / 'visualizations').exists() else 0} files)\")\n",
        "print(f\"  - summary_export.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Analyze features by role\n",
        "if features_path.exists():\n",
        "    df_features = pd.read_csv(features_path)\n",
        "    \n",
        "    # Group by role\n",
        "    if \"spectral.stable_rank\" in df_features.columns:\n",
        "        role_stats = df_features.groupby(\"role\").agg({\n",
        "            \"spectral.stable_rank\": [\"mean\", \"std\", \"max\"],\n",
        "            \"distribution.entropy\": [\"mean\", \"std\"] if \"distribution.entropy\" in df_features.columns else [],\n",
        "            \"energy.frobenius\": [\"mean\", \"std\"] if \"energy.frobenius\" in df_features.columns else []\n",
        "        })\n",
        "        \n",
        "        print(\"Statistics by role:\")\n",
        "        display(role_stats)\n",
        "        \n",
        "        # Plot feature distributions by role\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "        \n",
        "        # Stable rank by role\n",
        "        df_features.boxplot(column=\"spectral.stable_rank\", by=\"role\", ax=axes[0])\n",
        "        axes[0].set_title(\"Stable Rank by Role\")\n",
        "        axes[0].set_xlabel(\"Role\")\n",
        "        axes[0].set_ylabel(\"Stable Rank\")\n",
        "        \n",
        "        # Entropy by role\n",
        "        if \"distribution.entropy\" in df_features.columns:\n",
        "            df_features.boxplot(column=\"distribution.entropy\", by=\"role\", ax=axes[1])\n",
        "            axes[1].set_title(\"Entropy by Role\")\n",
        "            axes[1].set_xlabel(\"Role\")\n",
        "            axes[1].set_ylabel(\"Entropy\")\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(OUTPUT_DIR / \"role_analysis.png\", dpi=150, bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"Features CSV not available for custom analysis\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
